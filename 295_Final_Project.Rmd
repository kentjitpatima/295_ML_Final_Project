---
title: '295 ML for DL: Final Project'
author: "Kent Jitpatima"
date: "5/31/2022"
output: html_document
---

```{r setup, include=FALSE}
library(randomForest)
library(caret)
library(ggplot2)
library(lubridate)
library(scales)
library(gridExtra)
library(tidyverse)
library(rpart)
library(rpart.plot)
```

# 1 Data Wrangling

## 1.1 Load Data and Clean
```{r}
#Load Data Set
bank_data <- read.csv('bank-full.csv', sep = ';')
```


```{r}
#Clean up Data set, turn variables into tidyverse style, for Random Forest
#Change yes/no to numeric binary outcomes
#Change character inputs to Factors for possible logistic Regression
bank_data <- bank_data %>% 
  mutate(y = case_when(y == 'yes' ~ 1,
                       y == 'no' ~ 0)) %>% 
  mutate(loan = case_when(loan == 'yes' ~ 1,
                          loan == 'no' ~ 0)) %>% 
  mutate(housing = case_when(housing == 'yes' ~ 1, 
                             housing == 'no' ~ 0)) %>% 
  mutate(default = case_when(default == 'yes' ~ 1, 
                             default == 'no' ~ 0)) 
  
fact_cols <- c('job','marital','education','contact','month','poutcome','loan','housing','default','y')
bank_data[fact_cols] <- lapply(bank_data[fact_cols], factor)

```

## 1.2 Overview of Data 
```{r}
#Data Overview 
head(bank_data)
str(bank_data)
```
## 1.3 Create Train/Test Split
```{r}
#Data Partitioning into train/split
#Will use a 70%/30% split
set.seed(123)
split_index <- sample(2,nrow(bank_data), replace = TRUE, prob = c(0.7,0.3))
train <- bank_data[split_index == 1,]
test <- bank_data[split_index == 2,]
```

# 2 Decision Tree Model

## 2.1 Create Model and Plot

```{r}
library(tree)
library(rattle)
library(rpart.plot)
library(RColorBrewer)

dt <- rpart(y~.-duration-month-day, 
            data = train, 
            method = 'class',
            minsplit = 3,
            minbucket = 2)
```

## 2.2 Predict on Validation Set

```{r}
#Predict training data and validation data with Decision Tree Model
predict_train_dt <- predict(dt, train, type = 'class')
predict_test_dt <- predict(dt, test, type = 'class')
```

```{r}
#Create Confusion Table 
confusion_train_dt <- table(train$y, predict_train_dt)
confusion_train_acc <- sum(diag(confusion_train_dt)) / sum(confusion_train_dt)
confusion_train_dt
print(paste('Accuracy for train', confusion_train_acc))
```

```{r}
confusion_test_dt <- table(test$y, predict_test_dt)
confusion_test_acc <- sum(diag(confusion_test_dt)) / sum(confusion_test_dt)
confusion_test_dt
print(paste('Accuracy for test', confusion_test_acc))
```

## 2.3 Tune Hyperparameters

```{r}

```


# 3 Random Forest Model

## 3.1 Create Model 
```{r}
rf <- randomForest(y~.-duration-month-day, data=train , mtry = sqrt(13))
```

## 3.2 Display Diagnostics
```{r}
print(rf)
```

## 3.3 Evaluate Initial Random Forest Model
```{r}
#Create Confusion Matrices from testing data
p1 <- predict(rf, train)
confusionMatrix(p1, train$y)

#Prediction & Confusion MAtrix - test data
p2 <- predict(rf, test)
confusionMatrix(p2, test$y)
p2
```
```{r}
varImpPlot(rf)
```
## 3.4 Tune Hyperparameters with caret
```{r}
#Tuning using Caret
#might use ranger instead, maybe just cv
set.seed(123)
control <- trainControl(method="cv", number = 5, search="random")
mtry <- sqrt(13)
rf_random <- train(y~., data=bank_data, method="rf", metric="Accuracy", tuneLength = 4, trControl=control)
print(rf_random)
plot(rf_random)
```
## 3.5 New Tuned Model

```{r}
rf2 <- randomForest(y~.-duration-month-day, data=train, mtry = 13)

print(rf2)
```

```{r}
#Create Confusion Matrices from testing data
p3 <- predict(rf2, train)
confusionMatrix(p1, train$y)

#Prediction & Confusion MAtrix - test data
p4 <- predict(rf2, test)
confusionMatrix(p2, test$y)
```

# 4 Gradient Boosting Random Forest(XGBOOST)

## 4.1 Wrangle Data for use in Xgboost
```{r}
library(xgboost)
bank_data_xg <- bank_data %>% 
  select(-duration, -month, -day, -y)

bank_data_xg <- bank_data_xg %>%
  sapply(., as.numeric) %>% 
  as.matrix()

train_xg <- bank_data_xg[split_index == 1,]
test_xg <- bank_data_xg[split_index == 2,]

train_xg_label <- bank_data %>% 
  select(y) %>% 
  as.matrix() %>% 
  .[split_index == 1,] %>% 
  as.numeric()

test_xg_label <- bank_data %>% 
  select(y) %>% 
  as.matrix() %>% 
  .[split_index == 2,] %>% 
  as.numeric()
```

```{r}
dtrain <- xgb.DMatrix(data = train_xg, label= train_xg_label)
dtest <- xgb.DMatrix(data = test_xg, label= test_xg_label)
```

## 4.2 Create Model
```{r, results = hide}
boost_rf <- xgboost(data = dtrain, # the data   
                    nround = 150000, # max number of boosting iterations
                    objective = "binary:logistic",
                    print.every.n = 1000)
```
```{r}
```


```{r}
summary(boost_rf)
```

## 4.3 Predict Validation 
```{r}
pred_xg <- as.numeric(predict(boost_rf,dtest)>0.5)
observed = getinfo(dtest, "label")
cm_xg <- confusionMatrix(table(pred_xg,observed))
cm_xg
```
## 4.4 Tune Hyperparameters

```{r}
#Default Parameters
params <- list(booster = "gbtree",
               objective = "binary:logistic",
               eta=0.3, 
               gamma=0, 
               max_depth=6, 
               min_child_weight=1, 
               subsample=1, 
               colsample_bytree=1)

xgbcv <- xgb.cv( params = params,
                 data = dtrain, 
                 nrounds = 150000,
                 nfold = 5, 
                 showsd = T, 
                 stratified = T, 
                 print.every.n = 1000,
                 early.stop.round = 5000, 
                 maximize = T)
```
```{r}
xgbcv$best_iteration
```


```{r}
#load libraries
library(data.table)
library(mlr)
#create tasks
traintask <- makeClassifTask (data = train,target = "y")
testtask <- makeClassifTask (data = test,target = "y")

#do one hot encoding
traintask <- createDummyFeatures (obj = traintask) 
testtask <- createDummyFeatures (obj = testtask)

#create learner
lrn <- makeLearner("classif.xgboost",
                   predict.type = "response")

lrn$par.vals <- list( objective="binary:logistic", 
                      eval_metric="error",
                      nrounds=150000, 
                      eta=0.1)

#set parameter space
params <- makeParamSet( makeDiscreteParam("booster",
                                          values = c("gbtree","gblinear")),
                        makeIntegerParam("max_depth",
                                         lower = 3L,
                                         upper = 10L),
                        makeNumericParam("min_child_weight",
                                         lower = 1L,
                                         upper = 10L),
                        makeNumericParam("subsample",
                                         lower = 0.5,
                                         upper = 1),
                        makeNumericParam("colsample_bytree",
                                         lower = 0.5,
                                         upper = 1))
#set re sampling strategy
rdesc <- makeResampleDesc("CV",stratify = T,iters=5L)

#search strategy
ctrl <- makeTuneControlRandom(maxit = 10L)
```


```{r}
#set parallel backend
library(parallel)
library(parallelMap) 
parallelStartSocket(cpus = detectCores())

#parameter tuning
mytune <- tuneParams(learner = lrn, 
                     task = traintask, 
                     resampling = rdesc, 
                     measures = acc, 
                     par.set = params, 
                     control = ctrl, 
                     show.info = T)
mytune$y 
```


```{r}
#set hyperparameters
lrn_tune <- setHyperPars(lrn,par.vals = mytune$x)

#train model
xgmodel <- train(learner = lrn_tune,task = traintask)

#predict model
xgpred <- predict(xgmodel,testtask)
```

```{r}
confusionMatrix(xgpred$data$response,xgpred$data$truth)
```

